{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d97fff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from statistics import mean, variance\n",
    "from scipy import stats \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from feature_engine.selection import DropCorrelatedFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np \n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV , KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor \n",
    "import plotly.graph_objects as go\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from numpy import arange\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e0ddcc",
   "metadata": {},
   "source": [
    "###### IMPORT DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43988b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('trainDataset.xls')\n",
    "df = df.drop('ID', axis=1)\n",
    "df = df.drop('pCR (outcome)', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245827f3",
   "metadata": {},
   "source": [
    "###### DATA INSPECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4f20688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Columns: 118 entries, RelapseFreeSurvival (outcome) to original_ngtdm_Strength\n",
      "dtypes: float64(108), int64(10)\n",
      "memory usage: 368.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "df.describe()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1d1df3",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2436ace7",
   "metadata": {},
   "source": [
    "imputing missing values with median "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7aae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=999, strategy='median')\n",
    "# Fitting the data to the imputer object\n",
    "imputer = imp.fit(df)\n",
    "imputed_df = imputer.transform(df)\n",
    "imputed_df = pd.DataFrame(imputed_df, columns = df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b6b709",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_47784\\2316405402.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscaled_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_test_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalisation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    " scaled_x, scaled_test_x = normalisation(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_correlate_df(df):\n",
    "    X_aux = df.copy()\n",
    "    for col in df.columns:\n",
    "        X_aux[col] = df[col].sample(len(df)).values\n",
    "    return X_aux\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(scaled_x)\n",
    "original_variance = pca.explained_variance_ratio_\n",
    "\n",
    "N_permutations = 1000\n",
    "variance = np.zeros((N_permutations, len(scaled_x.columns)))\n",
    "\n",
    "for i in range(N_permutations):\n",
    "    X_aux = de_correlate_df(scaled_x)\n",
    "    pca.fit(X_aux)\n",
    "    variance[i, :] = pca.explained_variance_ratio_\n",
    "    \n",
    "p_val = np.sum(variance > original_variance, axis=0) / N_permutations\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=[f'PC{i}' for i in range(len(df.columns))], y=p_val, name='p-value on significance'))\n",
    "fig.update_layout(title=\"PCA Permutation Test p-values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db2c485",
   "metadata": {},
   "source": [
    "###### feature normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f17be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation(x_train, x_test):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_x = scaler.fit_transform(x_train)\n",
    "    scaled_x = pd.DataFrame(scaled_x)\n",
    "    scaled_test_x = scaler.fit_transform(x_test)\n",
    "    return scaled_x , scaled_test_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5281173",
   "metadata": {},
   "source": [
    "###### Parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc22530",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1b1cd",
   "metadata": {},
   "source": [
    "###### param tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1885dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGB_param_tuning(cv_inner):\n",
    "    xgb1 = XGBRegressor()\n",
    "    parameters = {\n",
    "                  'learning_rate': [.03, 0.05, .07, 0.1],\n",
    "                  'max_depth': [5, 6, 7],\n",
    "                  'subsample': [0.5, 0.7,1.0],\n",
    "                  'colsample_bytree': [0.7],\n",
    "                  'n_estimators': [100, 250, 500]\n",
    "    }\n",
    "      \n",
    "    xgb_grid = GridSearchCV(xgb1,\n",
    "                            parameters,\n",
    "                            cv = cv_inner, scoring='neg_mean_absolute_error',\n",
    "                            n_jobs = -1,\n",
    "                            verbose=True)\n",
    "    xgb_grid.fit(x_PCA,y_train)\n",
    "\n",
    "    return xgb_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57813a90",
   "metadata": {},
   "source": [
    "### lightbgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73ec4b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search  hyperparameter tuning\n",
    "def lgm_param(cv_inner):\n",
    "    parameters = {\n",
    "        'boosting': ['gbdt', 'dart','goss' ],\n",
    "        'lambda_l1':[0.01, 0.1, 0.5],\n",
    "       'num_leaves':[ 7, 15, 31  ],\n",
    "       'max_depth' :[ 10,15,25],\n",
    "       'min_data_in_leaf':[15,25]\n",
    "    }\n",
    "    \n",
    "    lgb_model = lgb.LGBMRegressor()\n",
    "\n",
    "    gsearch_lgb = GridSearchCV(lgb_model,scoring = 'neg_mean_absolute_error', param_grid = parameters, n_jobs=-1,verbose=10)\n",
    "    gsearch_lgb.fit(x_PCA,y_train)\n",
    "    return gsearch_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f493e",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfbfa5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter tuning for support vector machine\n",
    "def svr_param(cv_inner):\n",
    "    SVR_param_grid = {'C':[0.01, 0.1, 1,10],'kernel':['linear','rbf','poly'],'gamma':[0.001,0.01,0.1,1,10], 'epsilon':[0.01,0.1,1,10]}\n",
    "\n",
    "    #Find the best parameters\n",
    "    svr_grid = GridSearchCV(SVR(), SVR_param_grid, cv=cv_inner, scoring='neg_mean_absolute_error')\n",
    "    svr_grid.fit(x_PCA, np.ravel(y_train))\n",
    "    return svr_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffa7d45",
   "metadata": {},
   "source": [
    "### Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e17c8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter tuning for decision tree\n",
    "def dt_param(cv_inner):\n",
    "    param = {\n",
    "        'splitter': ['best','random'],\n",
    "        'max_depth': [1,3,5,7,9,11,12],\n",
    "        'min_samples_leaf': [1,2,3,4,5,6,7,8,9,10],\n",
    "        'min_weight_fraction_leaf':[0.1,0.2,0.3,0.4,0.5],\n",
    "        'max_features':[\"auto\",\"log2\",\"sqrt\",None],\n",
    "        'max_leaf_nodes':[None,10,20,30,40,50,60,70,80,90]\n",
    "            }\n",
    "\n",
    "    #define grid search\n",
    "    tree_grid = GridSearchCV(DecisionTreeRegressor(), param_grid = param, cv = cv_inner,n_jobs = -1, verbose = 3, scoring = 'neg_mean_absolute_error' )\n",
    "    tree_grid.fit(x_PCA,y_train)\n",
    "    return tree_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8efabb",
   "metadata": {},
   "source": [
    "### Bayesian Ridge Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94260018",
   "metadata": {},
   "outputs": [],
   "source": [
    " def br_param(cv_inner): \n",
    "    model = BayesianRidge()\n",
    "    # define model evaluation method\n",
    "    cv =KFold(n_splits = 7)\n",
    "    # define grid\n",
    "    grid = {'alpha_init':[0.3,0.5,0.8,1.0] , 'lambda_init':[1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-9]}\n",
    "\n",
    "    # define search\n",
    "    search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv_inner, n_jobs=-1)\n",
    "    # perform the search\n",
    "    results = search.fit(x_PCA, y_train)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59bcdcd",
   "metadata": {},
   "source": [
    "### NESTED CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d6e39",
   "metadata": {},
   "source": [
    "###### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9039832e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalisation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33920\\3850898317.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mscaled_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_test_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalisation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mx_PCA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalisation' is not defined"
     ]
    }
   ],
   "source": [
    "rfs = np.array(imputed_df['RelapseFreeSurvival (outcome)'])\n",
    "features = np.array(imputed_df.drop('RelapseFreeSurvival (outcome)', axis =1 ))\n",
    "\n",
    "cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in cv_outer.split(features):\n",
    "    # split data\n",
    "    x_train, x_test = features[train_ix, :], features[test_ix, :]\n",
    "    y_train, y_test = rfs[train_ix], rfs[test_ix]\n",
    "    cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    x_train = pd.DataFrame(x_train)\n",
    "    \n",
    "    scaled_x, scaled_test_x = normalisation(x_train, x_test)\n",
    "    pca = PCA(n_components=11)\n",
    "    x_PCA = pca.fit_transform(scaled_x)\n",
    "    x_test = pca.fit_transform(scaled_test_x)\n",
    "\n",
    "    search = XGB_param_tuning(cv_inner)\n",
    "     # get the best performing model fit on the whole training set\n",
    "    best_model = search.best_estimator_\n",
    "    pred = best_model.predict(x_test)\n",
    "    mae = mean_absolute_error(y_test,pred)\n",
    "    outer_results.append(mae)\n",
    "    print('>mae=%.3f, est=%.3f, cfg=%s' % (mae, search.best_score_, search.best_params_))\n",
    "print('Mean Absolute Error after parameter tuning: {:.2f}'.format(mean(outer_results)))\n",
    "print('variance after parameter tuning: {:.2f}'.format(variance(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c0066",
   "metadata": {},
   "source": [
    "###### LIGHT GBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3a9baae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "[LightGBM] [Warning] boosting is set=goss, boosting_type=gbdt will be ignored. Current value: boosting=goss\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "-------------------------------------------------\n",
      ">mae=21.364, est=-22.199, cfg={'boosting': 'goss', 'lambda_l1': 0.01, 'max_depth': 10, 'min_data_in_leaf': 25, 'num_leaves': 7}\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "-------------------------------------------------\n",
      ">mae=19.905, est=-23.346, cfg={'boosting': 'dart', 'lambda_l1': 0.5, 'max_depth': 10, 'min_data_in_leaf': 15, 'num_leaves': 7}\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "[LightGBM] [Warning] boosting is set=goss, boosting_type=gbdt will be ignored. Current value: boosting=goss\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "-------------------------------------------------\n",
      ">mae=22.434, est=-23.261, cfg={'boosting': 'goss', 'lambda_l1': 0.01, 'max_depth': 10, 'min_data_in_leaf': 25, 'num_leaves': 7}\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "[LightGBM] [Warning] boosting is set=goss, boosting_type=gbdt will be ignored. Current value: boosting=goss\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "-------------------------------------------------\n",
      ">mae=25.488, est=-22.430, cfg={'boosting': 'goss', 'lambda_l1': 0.1, 'max_depth': 10, 'min_data_in_leaf': 25, 'num_leaves': 7}\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "[LightGBM] [Warning] boosting is set=goss, boosting_type=gbdt will be ignored. Current value: boosting=goss\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "-------------------------------------------------\n",
      ">mae=19.592, est=-23.531, cfg={'boosting': 'goss', 'lambda_l1': 0.01, 'max_depth': 10, 'min_data_in_leaf': 25, 'num_leaves': 15}\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "[LightGBM] [Warning] boosting is set=goss, boosting_type=gbdt will be ignored. Current value: boosting=goss\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "-------------------------------------------------\n",
      ">mae=24.448, est=-22.059, cfg={'boosting': 'goss', 'lambda_l1': 0.01, 'max_depth': 10, 'min_data_in_leaf': 25, 'num_leaves': 15}\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "[LightGBM] [Warning] boosting is set=goss, boosting_type=gbdt will be ignored. Current value: boosting=goss\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "-------------------------------------------------\n",
      ">mae=26.681, est=-22.639, cfg={'boosting': 'goss', 'lambda_l1': 0.01, 'max_depth': 10, 'min_data_in_leaf': 25, 'num_leaves': 15}\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "[LightGBM] [Warning] boosting is set=goss, boosting_type=gbdt will be ignored. Current value: boosting=goss\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=15, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=15\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.1, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.1\n",
      "-------------------------------------------------\n",
      ">mae=22.029, est=-23.529, cfg={'boosting': 'goss', 'lambda_l1': 0.1, 'max_depth': 10, 'min_data_in_leaf': 15, 'num_leaves': 7}\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "[LightGBM] [Warning] boosting is set=goss, boosting_type=gbdt will be ignored. Current value: boosting=goss\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "-------------------------------------------------\n",
      ">mae=26.686, est=-22.546, cfg={'boosting': 'goss', 'lambda_l1': 0.01, 'max_depth': 10, 'min_data_in_leaf': 25, 'num_leaves': 7}\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "[LightGBM] [Warning] boosting is set=goss, boosting_type=gbdt will be ignored. Current value: boosting=goss\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=25\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.01, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.01\n",
      "-------------------------------------------------\n",
      ">mae=21.881, est=-22.637, cfg={'boosting': 'goss', 'lambda_l1': 0.01, 'max_depth': 10, 'min_data_in_leaf': 25, 'num_leaves': 7}\n",
      "Mean Absolute Error after parameter tuning: 23.05\n",
      "variance after parameter tuning: 6.86\n"
     ]
    }
   ],
   "source": [
    "rfs = np.array(imputed_df['RelapseFreeSurvival (outcome)'])\n",
    "features = np.array(imputed_df.drop('RelapseFreeSurvival (outcome)', axis =1 ))\n",
    "\n",
    "cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in cv_outer.split(features):\n",
    "    # split data\n",
    "    x_train, x_test = features[train_ix, :], features[test_ix, :]\n",
    "    y_train, y_test = rfs[train_ix], rfs[test_ix]\n",
    "    cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    x_train = pd.DataFrame(x_train)\n",
    "    scaled_x, scaled_test_x = normalisation(x_train, x_test)\n",
    "    pca = PCA(n_components=11)\n",
    "    x_PCA = pca.fit_transform(scaled_x)\n",
    "    x_test = pca.fit_transform(scaled_test_x)\n",
    "\n",
    "    search = lgm_param(cv_inner)\n",
    "     # get the best performing model fit on the whole training set\n",
    "    best_model = search.best_estimator_\n",
    "    pred = best_model.predict(x_test)\n",
    "    mae = mean_absolute_error(y_test,pred)\n",
    "    outer_results.append(mae)\n",
    "    print('-------------------------------------------------')\n",
    "    print('>mae=%.3f, est=%.3f, cfg=%s' % (mae, search.best_score_, search.best_params_))\n",
    "print('Mean Absolute Error after parameter tuning: {:.2f}'.format(mean(outer_results)))\n",
    "print('variance after parameter tuning: {:.2f}'.format(variance(outer_results)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b00aed",
   "metadata": {},
   "source": [
    "###### BAYESIAN RIDGE REGRESSION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04d2f143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      ">mae=20.671, est=-20.904, cfg={'alpha_init': 0.5, 'lambda_init': 1e-09}\n",
      "-------------------------------------------------\n",
      ">mae=21.239, est=-20.920, cfg={'alpha_init': 0.3, 'lambda_init': 0.1}\n",
      "-------------------------------------------------\n",
      ">mae=19.198, est=-20.849, cfg={'alpha_init': 0.3, 'lambda_init': 1e-09}\n",
      "-------------------------------------------------\n",
      ">mae=25.343, est=-20.293, cfg={'alpha_init': 0.3, 'lambda_init': 1e-09}\n",
      "-------------------------------------------------\n",
      ">mae=16.305, est=-21.347, cfg={'alpha_init': 0.3, 'lambda_init': 0.1}\n",
      "-------------------------------------------------\n",
      ">mae=24.094, est=-20.352, cfg={'alpha_init': 0.3, 'lambda_init': 1e-09}\n",
      "-------------------------------------------------\n",
      ">mae=24.909, est=-20.523, cfg={'alpha_init': 0.3, 'lambda_init': 0.1}\n",
      "-------------------------------------------------\n",
      ">mae=17.773, est=-21.039, cfg={'alpha_init': 0.3, 'lambda_init': 1e-09}\n",
      "-------------------------------------------------\n",
      ">mae=21.081, est=-20.634, cfg={'alpha_init': 0.3, 'lambda_init': 1e-09}\n",
      "-------------------------------------------------\n",
      ">mae=21.545, est=-20.647, cfg={'alpha_init': 0.3, 'lambda_init': 0.1}\n",
      "Mean Absolute Error after parameter tuning: 21.22\n",
      "variance after parameter tuning: 8.83\n"
     ]
    }
   ],
   "source": [
    "rfs = np.array(imputed_df['RelapseFreeSurvival (outcome)'])\n",
    "features = np.array(imputed_df.drop('RelapseFreeSurvival (outcome)', axis =1 ))\n",
    "\n",
    "cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in cv_outer.split(features):\n",
    "    # split data\n",
    "    x_train, x_test = features[train_ix, :], features[test_ix, :]\n",
    "    y_train, y_test = rfs[train_ix], rfs[test_ix]\n",
    "    cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    x_train = pd.DataFrame(x_train)\n",
    "    \n",
    "    scaled_x, scaled_test_x = normalisation(x_train, x_test)\n",
    "    pca = PCA(n_components=11)\n",
    "    x_PCA = pca.fit_transform(scaled_x)\n",
    "    x_test = pca.fit_transform(scaled_test_x)\n",
    "\n",
    "    search = br_param(cv_inner)\n",
    "     # get the best performing model fit on the whole training set\n",
    "    best_model = search.best_estimator_\n",
    "    pred = best_model.predict(x_test)\n",
    "    mae = mean_absolute_error(y_test,pred)\n",
    "    outer_results.append(mae)\n",
    "    print('-------------------------------------------------')\n",
    "    print('>mae=%.3f, est=%.3f, cfg=%s' % (mae, search.best_score_, search.best_params_))\n",
    "print('Mean Absolute Error after parameter tuning: {:.2f}'.format(mean(outer_results)))\n",
    "print('variance after parameter tuning: {:.2f}'.format(variance(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f111b",
   "metadata": {},
   "source": [
    "###### DECISION TREE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14f4b943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 28000 candidates, totalling 140000 fits\n",
      "-------------------------------------------------\n",
      ">mae=21.455, est=-20.042, cfg={'max_depth': 1, 'max_features': 'sqrt', 'max_leaf_nodes': 30, 'min_samples_leaf': 3, 'min_weight_fraction_leaf': 0.2, 'splitter': 'best'}\n",
      "Fitting 5 folds for each of 28000 candidates, totalling 140000 fits\n",
      "-------------------------------------------------\n",
      ">mae=20.717, est=-20.291, cfg={'max_depth': 12, 'max_features': 'sqrt', 'max_leaf_nodes': 50, 'min_samples_leaf': 9, 'min_weight_fraction_leaf': 0.3, 'splitter': 'best'}\n",
      "Fitting 5 folds for each of 28000 candidates, totalling 140000 fits\n",
      "-------------------------------------------------\n",
      ">mae=20.169, est=-20.383, cfg={'max_depth': 7, 'max_features': None, 'max_leaf_nodes': 20, 'min_samples_leaf': 10, 'min_weight_fraction_leaf': 0.1, 'splitter': 'random'}\n",
      "Fitting 5 folds for each of 28000 candidates, totalling 140000 fits\n",
      "-------------------------------------------------\n",
      ">mae=25.039, est=-19.747, cfg={'max_depth': 9, 'max_features': 'sqrt', 'max_leaf_nodes': 30, 'min_samples_leaf': 7, 'min_weight_fraction_leaf': 0.2, 'splitter': 'best'}\n",
      "Fitting 5 folds for each of 28000 candidates, totalling 140000 fits\n",
      "-------------------------------------------------\n",
      ">mae=16.937, est=-20.663, cfg={'max_depth': 11, 'max_features': 'auto', 'max_leaf_nodes': 70, 'min_samples_leaf': 4, 'min_weight_fraction_leaf': 0.1, 'splitter': 'random'}\n",
      "Fitting 5 folds for each of 28000 candidates, totalling 140000 fits\n",
      "-------------------------------------------------\n",
      ">mae=23.535, est=-19.813, cfg={'max_depth': 5, 'max_features': 'auto', 'max_leaf_nodes': 80, 'min_samples_leaf': 3, 'min_weight_fraction_leaf': 0.1, 'splitter': 'random'}\n",
      "Fitting 5 folds for each of 28000 candidates, totalling 140000 fits\n",
      "-------------------------------------------------\n",
      ">mae=24.371, est=-19.734, cfg={'max_depth': 11, 'max_features': 'log2', 'max_leaf_nodes': 50, 'min_samples_leaf': 3, 'min_weight_fraction_leaf': 0.2, 'splitter': 'best'}\n",
      "Fitting 5 folds for each of 28000 candidates, totalling 140000 fits\n",
      "-------------------------------------------------\n",
      ">mae=18.452, est=-20.286, cfg={'max_depth': 12, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_weight_fraction_leaf': 0.4, 'splitter': 'best'}\n",
      "Fitting 5 folds for each of 28000 candidates, totalling 140000 fits\n",
      "-------------------------------------------------\n",
      ">mae=20.710, est=-19.902, cfg={'max_depth': 7, 'max_features': None, 'max_leaf_nodes': 80, 'min_samples_leaf': 1, 'min_weight_fraction_leaf': 0.1, 'splitter': 'random'}\n",
      "Fitting 5 folds for each of 28000 candidates, totalling 140000 fits\n",
      "-------------------------------------------------\n",
      ">mae=20.793, est=-20.173, cfg={'max_depth': 12, 'max_features': None, 'max_leaf_nodes': None, 'min_samples_leaf': 7, 'min_weight_fraction_leaf': 0.1, 'splitter': 'random'}\n",
      "Mean Absolute Error after parameter tuning: 21.22\n",
      "variance after parameter tuning: 6.41\n"
     ]
    }
   ],
   "source": [
    "rfs = np.array(imputed_df['RelapseFreeSurvival (outcome)'])\n",
    "features = np.array(imputed_df.drop('RelapseFreeSurvival (outcome)', axis =1 ))\n",
    "\n",
    "cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in cv_outer.split(features):\n",
    "    # split data\n",
    "    x_train, x_test = features[train_ix, :], features[test_ix, :]\n",
    "    y_train, y_test = rfs[train_ix], rfs[test_ix]\n",
    "    cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    x_train = pd.DataFrame(x_train)\n",
    "    scaled_x, scaled_test_x = normalisation(x_train, x_test)\n",
    "    pca = PCA(n_components=11)\n",
    "    x_PCA = pca.fit_transform(scaled_x)\n",
    "    x_test = pca.fit_transform(scaled_test_x)\n",
    "\n",
    "    search = dt_param(cv_inner)\n",
    "     # get the best performing model fit on the whole training set\n",
    "    best_model = search.best_estimator_\n",
    "    pred = best_model.predict(x_test)\n",
    "    mae = mean_absolute_error(y_test,pred)\n",
    "    outer_results.append(mae)\n",
    "    print('-------------------------------------------------')\n",
    "    print('>mae=%.3f, est=%.3f, cfg=%s' % (mae, search.best_score_, search.best_params_))\n",
    "print('Mean Absolute Error after parameter tuning: {:.2f}'.format(mean(outer_results)))\n",
    "print('variance after parameter tuning: {:.2f}'.format(variance(outer_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275004da",
   "metadata": {},
   "source": [
    "###### SUPPORT VECTOR MACHINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfs = np.array(imputed_df['RelapseFreeSurvival (outcome)'])\n",
    "features = np.array(imputed_df.drop('RelapseFreeSurvival (outcome)', axis =1 ))\n",
    "\n",
    "cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "outer_results = list()\n",
    "for train_ix, test_ix in cv_outer.split(features):\n",
    "    # split data\n",
    "    x_train, x_test = features[train_ix, :], features[test_ix, :]\n",
    "    y_train, y_test = rfs[train_ix], rfs[test_ix]\n",
    "    cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    \n",
    "    x_train = pd.DataFrame(x_train)\n",
    "    scaled_x, scaled_test_x = normalisation(x_train, x_test)\n",
    "    pca = PCA(n_components=11)\n",
    "    x_PCA = pca.fit_transform(scaled_x)\n",
    "    x_test = pca.fit_transform(scaled_test_x)\n",
    "\n",
    "    search = svr_param(cv_inner)\n",
    "     # get the best performing model fit on the whole training set\n",
    "    best_model = search.best_estimator_\n",
    "    pred = best_model.predict(x_test)\n",
    "    mae = mean_absolute_error(y_test,pred)\n",
    "    outer_results.append(mae)\n",
    "    print('-------------------------------------------------')\n",
    "    print('>mae=%.3f, est=%.3f, cfg=%s' % (mae, search.best_score_, search.best_params_))\n",
    "print('Mean Absolute Error after parameter tuning: {:.2f}'.format(mean(outer_results)))\n",
    "print('variance after parameter tuning: {:.2f}'.format(variance(outer_results)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
